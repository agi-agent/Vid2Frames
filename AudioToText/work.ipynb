{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: whisper_timestamped [-h] [-v] [--versions] [--model MODEL]\n",
      "                           [--model_dir MODEL_DIR] [--device DEVICE]\n",
      "                           [--backend {openai-whisper,transformers}]\n",
      "                           [--output_dir OUTPUT_DIR]\n",
      "                           [--output_format OUTPUT_FORMAT]\n",
      "                           [--task {transcribe,translate}]\n",
      "                           [--language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}]\n",
      "                           [--vad VAD]\n",
      "                           [--detect_disfluencies DETECT_DISFLUENCIES]\n",
      "                           [--recompute_all_timestamps RECOMPUTE_ALL_TIMESTAMPS]\n",
      "                           [--punctuations_with_words PUNCTUATIONS_WITH_WORDS]\n",
      "                           [--temperature TEMPERATURE] [--best_of BEST_OF]\n",
      "                           [--beam_size BEAM_SIZE] [--patience PATIENCE]\n",
      "                           [--length_penalty LENGTH_PENALTY]\n",
      "                           [--suppress_tokens SUPPRESS_TOKENS]\n",
      "                           [--initial_prompt INITIAL_PROMPT]\n",
      "                           [--condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT]\n",
      "                           [--fp16 FP16]\n",
      "                           [--temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK]\n",
      "                           [--compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD]\n",
      "                           [--logprob_threshold LOGPROB_THRESHOLD]\n",
      "                           [--no_speech_threshold NO_SPEECH_THRESHOLD]\n",
      "                           [--threads THREADS]\n",
      "                           [--compute_confidence COMPUTE_CONFIDENCE]\n",
      "                           [--verbose VERBOSE] [--plot] [--debug] [--accurate]\n",
      "                           [--efficient] [--naive]\n",
      "                           audio [audio ...]\n",
      "\n",
      "Transcribe a single audio with whisper and compute word timestamps\n",
      "\n",
      "positional arguments:\n",
      "  audio                 audio file(s) to transcribe\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --version         show version and exit\n",
      "  --versions            show versions (of whisper-timestamped and whisper) and\n",
      "                        exit\n",
      "  --model MODEL         name of the Whisper model to use. Examples: tiny.en,\n",
      "                        tiny, base.en, base, small.en, small, medium.en,\n",
      "                        medium, large-v1, large-v2, large-v3, large,\n",
      "                        large-v3-turbo, turbo (default: small)\n",
      "  --model_dir MODEL_DIR\n",
      "                        the path to save model files; uses ~/.cache/whisper by\n",
      "                        default (default: None)\n",
      "  --device DEVICE       device to use for PyTorch inference (default: cuda)\n",
      "  --backend {openai-whisper,transformers}\n",
      "                        Which backend to use (default: openai-whisper)\n",
      "  --output_dir OUTPUT_DIR, -o OUTPUT_DIR\n",
      "                        directory to save the outputs (default: None)\n",
      "  --output_format OUTPUT_FORMAT, -f OUTPUT_FORMAT\n",
      "                        Format(s) of the output file(s). Possible formats are:\n",
      "                        txt, vtt, srt, tsv, csv, json. Several formats can be\n",
      "                        specified by using commas (ex: \"json,vtt,srt\"). By\n",
      "                        default (\"all\"), all available formats will be\n",
      "                        produced (default: all)\n",
      "  --task {transcribe,translate}\n",
      "                        whether to perform X->X speech recognition\n",
      "                        ('transcribe') or X->English translation ('translate')\n",
      "                        (default: transcribe)\n",
      "  --language {af,am,ar,as,az,ba,be,bg,bn,bo,br,bs,ca,cs,cy,da,de,el,en,es,et,eu,fa,fi,fo,fr,gl,gu,ha,haw,he,hi,hr,ht,hu,hy,id,is,it,ja,jw,ka,kk,km,kn,ko,la,lb,ln,lo,lt,lv,mg,mi,mk,ml,mn,mr,ms,mt,my,ne,nl,nn,no,oc,pa,pl,ps,pt,ro,ru,sa,sd,si,sk,sl,sn,so,sq,sr,su,sv,sw,ta,te,tg,th,tk,tl,tr,tt,uk,ur,uz,vi,yi,yo,yue,zh,Afrikaans,Albanian,Amharic,Arabic,Armenian,Assamese,Azerbaijani,Bashkir,Basque,Belarusian,Bengali,Bosnian,Breton,Bulgarian,Burmese,Cantonese,Castilian,Catalan,Chinese,Croatian,Czech,Danish,Dutch,English,Estonian,Faroese,Finnish,Flemish,French,Galician,Georgian,German,Greek,Gujarati,Haitian,Haitian Creole,Hausa,Hawaiian,Hebrew,Hindi,Hungarian,Icelandic,Indonesian,Italian,Japanese,Javanese,Kannada,Kazakh,Khmer,Korean,Lao,Latin,Latvian,Letzeburgesch,Lingala,Lithuanian,Luxembourgish,Macedonian,Malagasy,Malay,Malayalam,Maltese,Mandarin,Maori,Marathi,Moldavian,Moldovan,Mongolian,Myanmar,Nepali,Norwegian,Nynorsk,Occitan,Panjabi,Pashto,Persian,Polish,Portuguese,Punjabi,Pushto,Romanian,Russian,Sanskrit,Serbian,Shona,Sindhi,Sinhala,Sinhalese,Slovak,Slovenian,Somali,Spanish,Sundanese,Swahili,Swedish,Tagalog,Tajik,Tamil,Tatar,Telugu,Thai,Tibetan,Turkish,Turkmen,Ukrainian,Urdu,Uzbek,Valencian,Vietnamese,Welsh,Yiddish,Yoruba}\n",
      "                        language spoken in the audio, specify None to perform\n",
      "                        language detection. (default: None)\n",
      "  --vad VAD             whether to run Voice Activity Detection (VAD) to\n",
      "                        remove non-speech segment before applying Whisper\n",
      "                        model (removes hallucinations). Can be: True, False,\n",
      "                        auditok, silero (default when vad=True), silero:3.1\n",
      "                        (or another version), or a list of timestamps in\n",
      "                        seconds (e.g. \"[(0.0, 3.50), (32.43, 36.43)]\"). Note:\n",
      "                        Some additional libraries might be needed (torchaudio\n",
      "                        and onnxruntime for silero, auditok for auditok).\n",
      "                        (default: False)\n",
      "  --detect_disfluencies DETECT_DISFLUENCIES\n",
      "                        whether to try to detect disfluencies, marking them as\n",
      "                        special words [*] (default: False)\n",
      "  --recompute_all_timestamps RECOMPUTE_ALL_TIMESTAMPS\n",
      "                        Do not rely at all on Whisper timestamps (Experimental\n",
      "                        option: did not bring any improvement, but could be\n",
      "                        useful in cases where Whipser segment timestamp are\n",
      "                        wrong by more than 0.5 seconds) (default: False)\n",
      "  --punctuations_with_words PUNCTUATIONS_WITH_WORDS\n",
      "                        whether to include punctuations in the words (default:\n",
      "                        True)\n",
      "  --temperature TEMPERATURE\n",
      "                        temperature to use for sampling (default: 0.0)\n",
      "  --best_of BEST_OF     number of candidates when sampling with non-zero\n",
      "                        temperature (default: None)\n",
      "  --beam_size BEAM_SIZE\n",
      "                        number of beams in beam search, only applicable when\n",
      "                        temperature is zero (default: None)\n",
      "  --patience PATIENCE   optional patience value to use in beam decoding, as in\n",
      "                        https://arxiv.org/abs/2204.05424, the default (1.0) is\n",
      "                        equivalent to conventional beam search (default: None)\n",
      "  --length_penalty LENGTH_PENALTY\n",
      "                        optional token length penalty coefficient (alpha) as\n",
      "                        in https://arxiv.org/abs/1609.08144, uses simple\n",
      "                        length normalization by default (default: None)\n",
      "  --suppress_tokens SUPPRESS_TOKENS\n",
      "                        comma-separated list of token ids to suppress during\n",
      "                        sampling; '-1' will suppress most special characters\n",
      "                        except common punctuations (default: -1)\n",
      "  --initial_prompt INITIAL_PROMPT\n",
      "                        optional text to provide as a prompt for the first\n",
      "                        window. (default: None)\n",
      "  --condition_on_previous_text CONDITION_ON_PREVIOUS_TEXT\n",
      "                        if True, provide the previous output of the model as a\n",
      "                        prompt for the next window; disabling may make the\n",
      "                        text inconsistent across windows, but the model\n",
      "                        becomes less prone to getting stuck in a failure loop\n",
      "                        (default: True)\n",
      "  --fp16 FP16           whether to perform inference in fp16; Automatic by\n",
      "                        default (True if GPU available, False otherwise)\n",
      "                        (default: None)\n",
      "  --temperature_increment_on_fallback TEMPERATURE_INCREMENT_ON_FALLBACK\n",
      "                        temperature to increase when falling back when the\n",
      "                        decoding fails to meet either of the thresholds below\n",
      "                        (default: 0.0)\n",
      "  --compression_ratio_threshold COMPRESSION_RATIO_THRESHOLD\n",
      "                        if the gzip compression ratio is higher than this\n",
      "                        value, treat the decoding as failed (default: 2.4)\n",
      "  --logprob_threshold LOGPROB_THRESHOLD\n",
      "                        if the average log probability is lower than this\n",
      "                        value, treat the decoding as failed (default: -1.0)\n",
      "  --no_speech_threshold NO_SPEECH_THRESHOLD\n",
      "                        if the probability of the <|nospeech|> token is higher\n",
      "                        than this value AND the decoding has failed due to\n",
      "                        `logprob_threshold`, consider the segment as silence\n",
      "                        (default: 0.6)\n",
      "  --threads THREADS     number of threads used by torch for CPU inference;\n",
      "                        supercedes MKL_NUM_THREADS/OMP_NUM_THREADS (default:\n",
      "                        0)\n",
      "  --compute_confidence COMPUTE_CONFIDENCE\n",
      "                        whether to compute confidence scores for words\n",
      "                        (default: True)\n",
      "  --verbose VERBOSE     whether to print out the progress and debug messages\n",
      "                        of Whisper (default: False)\n",
      "  --plot                plot word alignments (save the figures if an\n",
      "                        --output_dir is specified, otherwhise just show\n",
      "                        figures that have to be closed to continue) (default:\n",
      "                        False)\n",
      "  --debug               print some debug information about word alignment\n",
      "                        (default: False)\n",
      "  --accurate            Shortcut to use the same default option as in openai-\n",
      "                        whisper (best_of=5, beam_search=5,\n",
      "                        temperature_increment_on_fallback=0.2) (default: None)\n",
      "  --efficient           Shortcut to disable beam size and options that\n",
      "                        requires to sample several times, for an efficient\n",
      "                        decoding (default: None)\n",
      "  --naive               use naive approach, doing inference twice (once to get\n",
      "                        the transcription, once to get word timestamps and\n",
      "                        confidence scores). (default: False)\n"
     ]
    }
   ],
   "source": [
    "!whisper_timestamped --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transcribe_timestamped in module whisper_timestamped.transcribe:\n",
      "\n",
      "transcribe_timestamped(model, audio, language=None, task='transcribe', remove_punctuation_from_words=False, compute_word_confidence=True, include_punctuation_in_confidence=False, refine_whisper_precision=0.5, min_word_duration=0.02, plot_word_alignment=False, word_alignment_most_top_layers=None, remove_empty_words=False, use_backend_timestamps=False, seed=1234, vad=False, detect_disfluencies=False, trust_whisper_timestamps=True, naive_approach=False, temperature=0.0, best_of=None, beam_size=None, patience=None, length_penalty=None, compression_ratio_threshold=2.4, logprob_threshold=-1.0, no_speech_threshold=0.6, fp16=None, condition_on_previous_text=True, initial_prompt=None, suppress_tokens='-1', sample_len=None, verbose=False)\n",
      "    Transcribe an audio file using Whisper\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    model: Whisper\n",
      "        The Whisper model instance.\n",
      "    \n",
      "    audio: Union[str, np.ndarray, torch.Tensor]\n",
      "        The path to the audio file to open, or the audio waveform in 16kHz.\n",
      "    \n",
      "    language: str\n",
      "        The language to use for the transcription. If None, the language is detected automatically.\n",
      "    \n",
      "    task: str\n",
      "        The task to perform: either \"transcribe\" or \"translate\".\n",
      "    \n",
      "    remove_punctuation_from_words: bool\n",
      "        If False, words will be glued with the next punctuation mark (if any).\n",
      "        If True, there will be no punctuation mark in the `words[:][\"text\"]` list.\n",
      "        It only affects these strings; This has no influence on the computation of the word confidence, whatever the value of `include_punctuation_in_confidence` is.\n",
      "    \n",
      "    include_punctuation_in_confidence: bool\n",
      "        Whether to include proba of punctuation in the computation of the (previous) word confidence.\n",
      "    \n",
      "    compute_word_confidence: bool\n",
      "        Whether to compute word confidence.\n",
      "        If True, a finer confidence for each segment will be computed as well.\n",
      "    \n",
      "    vad: bool or str in [\"silero\", \"silero:3.1\", \"auditok\"] or list of start/end timestamps pairs corresponding to speech (ex: [(0.0, 3.50), (32.43, 36.43)])\n",
      "        Whether to perform voice activity detection (VAD) on the audio file, to remove silent parts before transcribing with Whisper model.\n",
      "        This should decrease hallucinations from the Whisper model.\n",
      "        When set to True, the default VAD algorithm is used (silero).\n",
      "        When set to a string, the corresponding VAD algorithm is used (silero, silero:3.1 or auditok).\n",
      "        Note that the library for the corresponding VAD algorithm must be installed.\n",
      "    \n",
      "    detect_disfluencies: bool\n",
      "        Whether to detect disfluencies (i.e. hesitations, filler words, repetitions, corrections, etc.) that Whisper model might have omitted in the transcription.\n",
      "        This should make the word timestamp prediction more accurate.\n",
      "        And probable disfluencies will be marked as special words \"[*]\".\n",
      "    \n",
      "    trust_whisper_timestamps: bool\n",
      "        Whether to rely on Whisper's timestamps to get approximative first estimate of segment positions (up to refine_whisper_precision).\n",
      "    \n",
      "    refine_whisper_precision: float\n",
      "        How much can we refine Whisper segment positions, in seconds. Must be a multiple of 0.02.\n",
      "    \n",
      "    min_word_duration: float\n",
      "        Minimum duration of a word, in seconds. If a word is shorter than this, timestamps will be adjusted.\n",
      "    \n",
      "    plot_word_alignment: bool\n",
      "        Whether to plot the word alignment for each segment. matplotlib must be installed to use this option.\n",
      "    \n",
      "    remove_empty_words: bool\n",
      "        Whether to remove words with no duration occuring at the end of segments (probable Whisper hallucinations).\n",
      "    \n",
      "    use_backend_timestamps: bool\n",
      "        Whether to use word timestamps provided by the backend (openai-whisper or transformers), instead of the ones computed by more complex heuristics of whisper-timestamped.\n",
      "    \n",
      "    seed: int\n",
      "        Random seed to use for temperature sampling, for the sake of reproducibility.\n",
      "        Choose None for unpredictable randomness.\n",
      "    \n",
      "    naive_approach: bool\n",
      "        Force the naive approach that consists in decoding twice the audio file, once to get the transcritpion and once with the decoded tokens to get the alignment.\n",
      "        Note that this approach is used anyway when beam_size is not None and/or when the temperature is a list with more than one element.\n",
      "    \n",
      "    temperature: float\n",
      "        Temperature for sampling.\n",
      "    \n",
      "    compression_ratio_threshold: float\n",
      "        If the gzip compression ratio is above this value, treat as failed.\n",
      "    \n",
      "    logprob_threshold: float\n",
      "        If the average log probability over sampled tokens is below this value, treat as failed.\n",
      "    \n",
      "    no_speech_threshold: float\n",
      "        If the no_speech probability is higher than this value AND the average log probability\n",
      "        over sampled tokens is below `logprob_threshold`, consider the segment as silent.\n",
      "    \n",
      "    condition_on_previous_text: bool\n",
      "        if True, the previous output of the model is provided as a prompt for the next window;\n",
      "        disabling may make the text inconsistent across windows, but the model becomes less prone to\n",
      "        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n",
      "    \n",
      "    initial_prompt: str\n",
      "        Optional text to provide as a prompt for the first window.\n",
      "    \n",
      "    suppress_tokens: str\n",
      "        Comma-separated list of token ids to suppress during sampling;\n",
      "        '-1' will suppress most special characters except common punctuations.\n",
      "    \n",
      "    verbose: bool\n",
      "        Whether to display the text being decoded to the console. If True, displays all the details,\n",
      "        If False, displays minimal details. If None, does not display anything\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and\n",
      "    the spoken language (\"language\"), which is detected when `decode_options[\"language\"]` is None.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import whisper_timestamped as whisper\n",
    "import csv\n",
    "help(whisper.transcribe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hydraallen/Desktop/Github/Others/anaconda3/envs/agent/lib/python3.10/site-packages/openai_whisper-20240930-py3.10.egg/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "100%|██████████| 96845/96845 [00:24<00:00, 3938.45frames/s]\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"../SampleVideo/Emacs_test_1.mp4\"\n",
    "base_name = audio_path.rsplit(\".\", 1)[0]\n",
    "audio = whisper.load_audio(audio_path)\n",
    "# model = whisper.load_model(\"NbAiLab/whisper-large-v2-nob\") #, device=\"cpu\"\n",
    "model = whisper.load_model(\"tiny\") #, device=\"cpu\"\n",
    "result = whisper.transcribe(model, audio, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_output = f\"{base_name}_sentence_timestamp.csv\"\n",
    "with open(sentence_output, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"start\", \"end\", \"text\"])\n",
    "    for segment in result[\"segments\"]:\n",
    "        writer.writerow([segment[\"start\"], segment[\"end\"], segment[\"text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_output = f\"{base_name}_word_timestamp.csv\"\n",
    "with open(word_output, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"segment_id\", \"word\", \"start\", \"end\", \"confidence\"])\n",
    "    for segment in result[\"segments\"]:\n",
    "        segment_id = segment[\"id\"]\n",
    "        for word in segment[\"words\"]:\n",
    "            writer.writerow([segment_id, word[\"text\"], word[\"start\"], word[\"end\"], word[\"confidence\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
